package edu.columbia.cs.ref.tool.collection.splitter.impl;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import edu.columbia.cs.ref.model.Dataset;
import edu.columbia.cs.ref.tool.collection.splitter.Splitter;
import edu.columbia.cs.ref.tool.io.Writable;

/**
 * This class is an implementation of the splitter interface for the K-fold cross-validation
 * method.
 * 
 * <br>
 * <br>
 * 
 * This splitter is parameterized by the number of splits that are desired. The files that are
 * generated by this splitter are called "train-i" and "test-i" where i is the number of the
 * fold.
 * 
 * @author      Pablo Barrio
 * @author		Goncalo Simoes
 * @version     0.1
 * @since       2011-09-27
 */
public class KFoldSplitter<E extends Writable> extends Splitter<E> {

	private int numberSplits;

	/**
	 * Constructor for the KFoldSplitter. It receives as input the number of splits that will
	 * be produced in the process
	 * 
	 * @param numberSplits number of folds produced by the method split
	 */
	public KFoldSplitter(int numberSplits){
		this.numberSplits=numberSplits;
	}

	/**
	 * This method is an implementation of the split method for the K-Fold cross validation
	 * method. In order to compute the splits what is done is the following:
	 * 
	 * <br>
	 * <br>
	 * 1) Create a list with all the elements of a dataset
	 * <br>
	 * <br>
	 * 2) Shuffle that list using the Collections.shuffle method from the java API
	 * <br>
	 * <br>
	 * 3) Create k buckets and scan the list putting each element of the list in one bucket
	 * at a time
	 * <br>
	 * <br>
	 * 4) Generate the fold files
	 * 
	 * @param dataset dataset to be splitted
	 * @param outputFolder folder where the fold files will be written
	 **/
	@Override
	public void split(Dataset<E> dataset, File outputFolder) {
		List<E> listE = new ArrayList<E>();
		for(E element : dataset){
			listE.add(element);
		}
		Collections.shuffle(listE);

		List<E>[] buckets = new List[numberSplits];
		for(int i=0; i<numberSplits; i++){
			buckets[i]=new ArrayList<E>();
		}

		int currentBucket=0;
		for(E element : listE){
			buckets[currentBucket].add(element);
			currentBucket=(currentBucket+1)%numberSplits;
		}

		for(int i=0; i<numberSplits; i++){
			List<E> trainingData = new ArrayList<E>();
			List<E> testingData = new ArrayList<E>();
			testingData.addAll(buckets[i]);
			for(int j=0; j<numberSplits; j++){
				if(j!=i){
					trainingData.addAll(buckets[j]);
				}
			}

			System.out.println("Split " + i);
			System.out.println("Training docs: " + trainingData.size());
			System.out.println("Testing docs: " + testingData.size());
			writeFile(trainingData, outputFolder, "/train-"+i);
			writeFile(testingData, outputFolder, "/test-"+i);
		}

	}

	private void writeFile(List<E> data, File directory, String file){
		try {	
			// Create file 
			FileWriter fstream;

			fstream = new FileWriter(directory.getAbsolutePath()+file);

			BufferedWriter out = new BufferedWriter(fstream);
			for(E s : data){
				out.write(s.getWritableValue() + "\n");
			}
			//Close the output stream
			out.close();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			System.exit(1);
		}
	}
}
